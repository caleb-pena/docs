---
title: "Create Chat Completion"
sidebarTitle: "Chat Request"
api: "POST https://api.ai21.com/studio/v1/chat/completions"
contentType: "application/json"
---

## Overview

The Jamba API provides access to a set of instruction-following chat models.\
This document describes the details for interacting with the chat model via the API endpoint and provides specifications for request and response structures.

## Authentication

This header is required for all API requests. It must include a Bearer token for authentication.\
Use your API key to generate the token.

**Format**:\
`Authorization: Bearer <your-api-key>`

---

## Request body

---

<ParamField body="model" type="string" required>
  The name of the model to use.\
  You can call our model without specifying a version by using the following model names:

  - `jamba-large`\\
  - `jamba-mini`

  For more information on the available model versions, [click here](/docs/jamba-foundation-models).
</ParamField>

<ParamField body="messages" type="array[object]" required>
  A list of messages representing the conversation history.
  The structure of the message object depends on the type:

  <Expandable title="more">
    <ParamField body="system message" type="object">
      An initial system message is optional but recommended to set the tone of the chat.

      <Expandable title="more">
        <ParamField body="role" type="string" required>
          The role of the entity that is creating the message.
        </ParamField>
        <ParamField body="content" type="string" required>
          The content of the message.
        </ParamField>
      </Expandable>
    </ParamField>
    <ParamField body="user message" type="object">
      Input provided by the user.

      <Expandable title="more">
        <ParamField body="role" type="string" required>
          The role of the entity that is creating the message, in this case the user.
        </ParamField>
        <ParamField body="content" type="string" required>
          The content of the message.
        </ParamField>
      </Expandable>
    </ParamField>
    <ParamField body="assistant message" type="object">
      Response generated by the model. Include this in your request to provide context for future answers.

      <Expandable title="more">
        <ParamField body="role" type="string" required>
          The role of the entity that is creating the message, in this case the assistant.
        </ParamField>
        <ParamField body="content" type="string" required>
          The content of the message.
        </ParamField>
        <ParamField body="tool_calls" type="array[object]">
          The function calls generated by the model, such as tool invocations.
        </ParamField>
        <ParamField body="id" type="string">
          The id of the tool call.
        </ParamField>
        <ParamField body="type" type="string">
          The type of tool.
        </ParamField>
        <ParamField body="function" type="object">
          The function invoked by the model.
        </ParamField>
        <ParamField body="name" type="string">
          The name of the function.
        </ParamField>
        <ParamField body="arguments" type="JSON string">
          The parameters of the function as a JSON schema.
        </ParamField>
      </Expandable>
    </ParamField>
    <ParamField body="tool message" type="object">
      Contains the output of a tool. Add the function output for user-implemented tools to enable a user-friendly model response. If included, ensure an assistant message with a tool_calls entry with a matching id exists.

      <Expandable title="more">
        <ParamField body="role" type="string" required>
          The role of the entity that is creating the message, in this case the tool.
        </ParamField>
        <ParamField body="content" type="string" required>
          The content of the message.
        </ParamField>
        <ParamField body="tool_call_id" type="string" required>
          The message is a response to this tool call.
        </ParamField>
      </Expandable>
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="tools" type="array[object]">
  A list of tools that the model can use when generating a response.\
  Currently, the only function type tools are supported.

  <Expandable title="more">
    <ParamField body="type" type="string" required>
      The type of tool. Currently, the only supported value is "function".
    </ParamField>
    <ParamField body="function" type="object" required>
      Describes a function to call. Currently, all functions must be described by the user; there are no built-in functions. An example function template is given below.

      <Expandable title="more">
        <ParamField body="name" type="string" required>
          The name of the function.
        </ParamField>
        <ParamField body="description" type="object" required>
          Provide a complete description of what the function does, what it returns, and any limitations.
        </ParamField>
        <ParamField body="parameter" type="object">
          Each function parameter has a name, a type ("string", "integer", "float", "array", "boolean", or "enum"), and a description.
        </ParamField>
      </Expandable>
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="documents" type="array[object]">
  The document parameter accepts a list of objects, each containing multiple fields.

  <Expandable title="more">
    <ParamField body="content" type="string" required>
      The content of this "document".
    </ParamField>
    <ParamField body="metadata" type="array[object]">
      Key-value pairs describing the document:

      <Expandable title="more">
        <ParamField body="key" type="string" required>
          Type of metadata, like 'author', 'date', 'url', etc. Should be things the model understands.
        </ParamField>
      </Expandable>
    </ParamField>
    <ParamField body="value" type="string" required>
      Value of the metadata.
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="response_format" type="object">
  An object defining the output format required from the model.\
  Setting it to `{ "type": "json_object" }` activates JSON mode, ensuring the generated message adheres to valid JSON structure.
</ParamField>

<ParamField body="max_tokens" type="integer">
  The maximum number of tokens the model can generate in its response.\
  For Jamba models, the maximum allowed value is 4096 tokens.
</ParamField>

<ParamField body="temperature" type="float">
  Controls the variety of responses provided. A higher value results in more diverse answers.\
  [More information](/v4.2/docs/large-language-models). Default: 0.4, Range: 0.0 – 2.0
</ParamField>

<ParamField body="top_p" type="float">
  Limit the pool of next tokens in each step to the top N percentile of possible tokens, where 1.0 means the pool of all possible tokens, and 0.01 means the pool of only the most likely next tokens. [More information](/v4.2/docs/large-language-models). Default: 1.0, Range: 0 \<= value \<=1.0
</ParamField>

<ParamField body="stop" type="array[string]">
  End the message when the model generates one of these strings. The stop sequence is not included in the generated message. Each sequence can be up to 64K long, and can contain newlines as `n` characters.

  <Accordion title="Show more">
    - **Single stop string with a word and a period**: "monkeys."
    - **Multiple stop strings and a newline**: ["cat", "dog", " .", "####", "\\n"]
  </Accordion>
</ParamField>

<ParamField body="n" type="integer">
  How many chat responses to generate. _Default:1, Range: 1 – 16_ **Notes**:

  - If `n > 1`, setting `temperature = 0` will fail because all answers are guaranteed to be duplicates.
  - `n` must be 1 when `stream = True`
</ParamField>

<ParamField body="stream" type="boolean">
  Stream results one token at a time using server-sent events. Useful for long results to avoid long wait times. If `True`, `n` must be 1. Must be `False` if using `tools`.
</ParamField>

<RequestExample>

```python Python
from ai21 import AI21Client
from ai21.models.chat.chat_message import SystemMessage, UserMessage, AssistantMessage

system = "You're a support engineer in a SaaS company"
messages = [
    SystemMessage(content=system, role="system"),
    UserMessage(content="Hello, I need help with a signup process.", role="user"),
    AssistantMessage(content="Hi Alice, I can help you with that. What seems to be the problem?", role="assistant"),
    UserMessage(content="I am having trouble signing up for your product with my Google account.", role="user"),
]

client = AI21Client()

response = client.chat.completions.create(
    messages=messages,
    model="jamba-mini",
    max_tokens=100,
    temperature=0.7,
    top_p=1.0,
    stop=["\n"],
)
```


```javascript Javascript
from ai21 import AI21Client
from ai21.models.chat.chat_message import SystemMessage, UserMessage, AssistantMessage

system = "You're a support engineer in a SaaS company"
messages = [
    SystemMessage(content=system, role="system"),
    UserMessage(content="Hello, I need help with a signup process.", role="user"),
    AssistantMessage(content="Hi Alice, I can help you with that. What seems to be the problem?", role="assistant"),
    UserMessage(content="I am having trouble signing up for your product with my Google account.", role="user"),
]

client = AI21Client()

response = client.chat.completions.create(
    messages=messages,
    model="jamba-mini",
    max_tokens=100,
    temperature=0.7,
    top_p=1.0,
    stop=["\n"],
)
```


```bash Curl
from ai21 import AI21Client
from ai21.models.chat.chat_message import SystemMessage, UserMessage, AssistantMessage

system = "You're a support engineer in a SaaS company"
messages = [
    SystemMessage(content=system, role="system"),
    UserMessage(content="Hello, I need help with a signup process.", role="user"),
    AssistantMessage(content="Hi Alice, I can help you with that. What seems to be the problem?", role="assistant"),
    UserMessage(content="I am having trouble signing up for your product with my Google account.", role="user"),
]

client = AI21Client()

response = client.chat.completions.create(
    messages=messages,
    model="jamba-mini",
    max_tokens=100,
    temperature=0.7,
    top_p=1.0,
    stop=["\n"],
)
```

</RequestExample>